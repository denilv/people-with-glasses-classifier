{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/NVME1TB/Projects/people-with-glasses-classifier\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torchvision as tv\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from modules.comp_tools import preprocessing_fn, ClsDataset\n",
    "from torch.utils.data import DataLoader as BaseDataLoader\n",
    "\n",
    "from modules.mobilenetv2 import MobileNetV2\n",
    "from modules.mobilenetv3 import mobilenetv3_large\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FOLDS = (0, 1, 2)\n",
    "VALID_FOLDS = (3,)\n",
    "TEST_FOLDS = (4,)\n",
    "# CONTINUE = 'logs/mobilenetv3-binary/checkpoints/last.pth'\n",
    "CONTINUE = 'logs/mobilenetv2-adam-binary/checkpoints/last.pth'\n",
    "\n",
    "MODE = 'multiclass'\n",
    "BINARY= True\n",
    "ACTIVATION = 'sigmoid'\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "TRAIN_IMAGES = 'data/crops/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all.csv')\n",
    "test_df = df[df.fold_num.isin(TEST_FOLDS)]\n",
    "valid_df = df[df.fold_num.isin(VALID_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_params = dict(\n",
    "    img_size=(120, 120),\n",
    "    img_prefix=TRAIN_IMAGES, \n",
    "    augmentations=None,\n",
    "    preprocess_img=preprocessing_fn,\n",
    "    mode=MODE,\n",
    "    binary=BINARY,\n",
    ")\n",
    "\n",
    "valid_dataset = ClsDataset(valid_df, **dataset_params)\n",
    "valid_dl = BaseDataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "test_dataset = ClsDataset(test_df, **dataset_params)\n",
    "test_dl = BaseDataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device=None, th=0.5, n_batches=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    accs = []\n",
    "    start = time.time()\n",
    "    c = 0\n",
    "    n = 0\n",
    "    total = len(data_loader)\n",
    "    if n_batches:\n",
    "        total = n_batches\n",
    "    pred_probas = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data_dict in tqdm(data_loader, total=total):\n",
    "            if n_batches and c >= n_batches:\n",
    "                break\n",
    "            image = data_dict['features'].to(device)\n",
    "            target = data_dict['targets'].to(device)\n",
    "            output = torch.sigmoid(model(image))\n",
    "            pred_probas.append(output.cpu().numpy())\n",
    "            true_labels.append(target.cpu().numpy().astype(int))\n",
    "            c += 1\n",
    "            n += target.size(0)\n",
    "    probas = np.concatenate(pred_probas)\n",
    "    pred = (probas > th).astype(int)\n",
    "    true = np.concatenate(true_labels)\n",
    "    \n",
    "    print(classification_report(true, pred))\n",
    "    elapsed_time = time.time() - start\n",
    "    fps = n / elapsed_time\n",
    "    time_per_image = 1 / fps\n",
    "    print(f'Elapsed time: {elapsed_time:0.2f}')\n",
    "    print(f'{time_per_image:0.5f} sec/img')\n",
    "    print(f'{fps:0.2f} img/sec (fps)')\n",
    "    return true, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(weights=None):\n",
    "    model = MobileNetV2(num_classes=1)\n",
    "    if weights:\n",
    "        state_dict = torch.load(weights)['model_state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(CONTINUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a24e8ae31c46ecb9f7431d368dedbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     43867\n",
      "           1       0.95      0.99      0.97      6785\n",
      "\n",
      "    accuracy                           0.99     50652\n",
      "   macro avg       0.97      0.99      0.98     50652\n",
      "weighted avg       0.99      0.99      0.99     50652\n",
      "\n",
      "Elapsed time: 19.74\n",
      "0.00039 sec/img\n",
      "2565.45 img/sec (fps)\n",
      "Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917219c3155a4930968c3176f7a9829b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     41692\n",
      "           1       0.97      0.99      0.98      8958\n",
      "\n",
      "    accuracy                           0.99     50650\n",
      "   macro avg       0.98      0.99      0.99     50650\n",
      "weighted avg       0.99      0.99      0.99     50650\n",
      "\n",
      "Elapsed time: 18.59\n",
      "0.00037 sec/img\n",
      "2725.24 img/sec (fps)\n"
     ]
    }
   ],
   "source": [
    "print('Validation')\n",
    "true, probas = evaluate(model, valid_dl, th=0.5)\n",
    "\n",
    "print('Test')\n",
    "_, _ = evaluate(model, test_dl, th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per dataset validation\n",
      "Dataset: celeba\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1171bcfc277f48b585e36395d6923ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     37801\n",
      "           1       0.91      0.97      0.94      2718\n",
      "\n",
      "    accuracy                           0.99     40519\n",
      "   macro avg       0.95      0.98      0.97     40519\n",
      "weighted avg       0.99      0.99      0.99     40519\n",
      "\n",
      "Elapsed time: 15.64\n",
      "0.00039 sec/img\n",
      "2590.22 img/sec (fps)\n",
      "Dataset: specface\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ac5488ad2a4192aaab3ac8a2d50420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00        64\n",
      "\n",
      "    accuracy                           1.00        64\n",
      "   macro avg       1.00      1.00      1.00        64\n",
      "weighted avg       1.00      1.00      1.00        64\n",
      "\n",
      "Elapsed time: 0.26\n",
      "0.00407 sec/img\n",
      "246.00 img/sec (fps)\n",
      "Dataset: sof\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6495910ed6934a6a87617edbfc577974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.96      0.98       484\n",
      "\n",
      "    accuracy                           0.96       484\n",
      "   macro avg       0.50      0.48      0.49       484\n",
      "weighted avg       1.00      0.96      0.98       484\n",
      "\n",
      "Elapsed time: 0.55\n",
      "0.00114 sec/img\n",
      "880.98 img/sec (fps)\n",
      "Dataset: meglass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denilv/anaconda3/envs/p37/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f13276aa314318ae04f5ea8c60392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      3891\n",
      "           1       0.99      1.00      1.00      5692\n",
      "\n",
      "    accuracy                           0.99      9583\n",
      "   macro avg       0.99      0.99      0.99      9583\n",
      "weighted avg       0.99      0.99      0.99      9583\n",
      "\n",
      "Elapsed time: 3.98\n",
      "0.00042 sec/img\n",
      "2409.08 img/sec (fps)\n"
     ]
    }
   ],
   "source": [
    "print('Per dataset validation')\n",
    "for dataset_type in test_df.dataset.unique():\n",
    "    print(f'Dataset: {dataset_type}')\n",
    "    sub_df = test_df[test_df.dataset==dataset_type]\n",
    "    sub_dataset = ClsDataset(sub_df, **dataset_params)\n",
    "    dl = BaseDataLoader(sub_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    evaluate(model, dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization Prepare: Inserting Observers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4e7c7f42f24ba999d9b0387be862ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      6007\n",
      "           1       0.88      0.97      0.92       393\n",
      "\n",
      "    accuracy                           0.99      6400\n",
      "   macro avg       0.94      0.98      0.96      6400\n",
      "weighted avg       0.99      0.99      0.99      6400\n",
      "\n",
      "Elapsed time: 76.28\n",
      "0.01192 sec/img\n",
      "83.90 img/sec (fps)\n",
      "Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "Quantized on valid dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denilv/anaconda3/envs/p37/lib/python3.7/site-packages/torch/quantization/observer.py:172: UserWarning: Must run observer before calling calculate_qparams.                           Returning default scale and zero point.\n",
      "  Returning default scale and zero point.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dd4c2efda54972ad19618399dd6511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     43867\n",
      "           1       0.95      0.99      0.97      6785\n",
      "\n",
      "    accuracy                           0.99     50652\n",
      "   macro avg       0.97      0.99      0.98     50652\n",
      "weighted avg       0.99      0.99      0.99     50652\n",
      "\n",
      "Elapsed time: 69.53\n",
      "0.00137 sec/img\n",
      "728.53 img/sec (fps)\n",
      "Quantized on test dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a121f67cd74f9590a63a4656f24e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=198.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     41692\n",
      "           1       0.97      0.99      0.98      8958\n",
      "\n",
      "    accuracy                           0.99     50650\n",
      "   macro avg       0.98      0.99      0.99     50650\n",
      "weighted avg       0.99      0.99      0.99     50650\n",
      "\n",
      "Elapsed time: 68.76\n",
      "0.00136 sec/img\n",
      "736.62 img/sec (fps)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]), array([[3.5459205e-05],\n",
       "        [3.1313755e-06],\n",
       "        [1.3195903e-02],\n",
       "        ...,\n",
       "        [1.5790707e-05],\n",
       "        [7.9624326e-05],\n",
       "        [1.1794723e-03]], dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.fuse_model()\n",
    "model.qconfig = torch.quantization.default_qconfig\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "evaluate(model, valid_dl, device='cpu', n_batches=25)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "print('Quantized on valid dataset')\n",
    "evaluate(model, valid_dl, device='cpu')\n",
    "print('Quantized on test dataset')\n",
    "evaluate(model, test_dl, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.jit.save(torch.jit.script(model), 'quantized-mobilenetv2-scripted.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.common import visualize\n",
    "from modules.utils import crop_img, resize_shortest_edge, open_img\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    with torch.no_grad():\n",
    "        tensor = preprocessing_fn(img)\n",
    "        tensor = torch.Tensor(tensor).unsqueeze(0)\n",
    "        logit = model(tensor)\n",
    "        prob = torch.sigmoid(logit).cpu().numpy()[0]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load('trained_models/quantized-mobilenetv2-scripted.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.20584455e-05]\n",
      "[2.3912426e-06]\n",
      "[0.00010427]\n",
      "[2.7078262e-05]\n",
      "[1.20584455e-05]\n",
      "[0.00452678]\n",
      "[7.0318465e-06]\n",
      "[0.13151582]\n",
      "[0.00010427]\n",
      "[4.7418968e-07]\n",
      "[0.00090094]\n",
      "[1.0648484e-06]\n",
      "[0.0020209]\n",
      "[6.20961e-07]\n",
      "[2.3912426e-06]\n",
      "[1.5790707e-05]\n",
      "[3.5459205e-05]\n",
      "[7.9624326e-05]\n",
      "[7.9624326e-05]\n",
      "[1.8260454e-06]\n"
     ]
    }
   ],
   "source": [
    "fld = 'data/cameos_dataset/without_glasses/'\n",
    "for fname in os.listdir(fld):\n",
    "    fp = osp.join(fld, fname)\n",
    "    img = open_img(fp)\n",
    "    crop = crop_img(img) / 255.\n",
    "    crop = cv2.resize(crop, (120, 120))\n",
    "    prob = predict(model, crop)\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99947447]\n",
      "[0.9973552]\n",
      "[0.9999999]\n",
      "[0.99982125]\n",
      "[0.99998426]\n",
      "[0.9999392]\n",
      "[0.99998796]\n",
      "[0.9999392]\n",
      "[0.9999908]\n",
      "[0.99999595]\n",
      "[0.7938518]\n",
      "[0.99992037]\n",
      "[0.9999999]\n",
      "[0.9979791]\n",
      "[0.99999964]\n",
      "[0.9999535]\n",
      "[0.99999595]\n",
      "[0.99999595]\n",
      "[0.99999976]\n",
      "[0.9998957]\n"
     ]
    }
   ],
   "source": [
    "fld = 'data/cameos_dataset/with_glasses/'\n",
    "for fname in os.listdir(fld):\n",
    "    fp = osp.join(fld, fname)\n",
    "    img = open_img(fp)\n",
    "    crop = crop_img(img) / 255.\n",
    "    crop = cv2.resize(crop, (120, 120))\n",
    "    prob = predict(model, crop)\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62f9693d6564fe3976fd90ec0c0e8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2718.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'open_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c6e8c55ac40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrop_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'open_img' is not defined"
     ]
    }
   ],
   "source": [
    "sub_df = test_df[(test_df.dataset == 'celeba') & (test_df.has_glasses == 1)]\n",
    "for ind, row in tqdm(sub_df.iterrows(), total=len(sub_df)):\n",
    "    fp = row.filename\n",
    "    img = open_img(fp)\n",
    "    crop = crop_img(img) / 255.\n",
    "    crop = cv2.resize(crop, (120, 120))\n",
    "    prob = predict(model, crop)\n",
    "    if prob < 0.85:\n",
    "        print(prob, ind, fp)\n",
    "        visualize(img=img, crop=crop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
